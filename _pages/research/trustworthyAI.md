---
title:
layout: default
permalink: /research/trustworthyAI
published: true
---
<!--
## Edge Intelligence

Gartner's hype cycle for artificial intelligence (AI) 2021, shown in figure 1, places the edge AI at the peak of inflated expectation, leaving the innovation trigger phase in only 12 months. Moreover, the IBM Institute for Business Value claims that the expected return on investment in green edge computing amounts to 10% in 2022. According to Gartner, however, there will still be room for further investments in edge AI, because it will steadily reach the Plateau of Productivity within a maximum of 5 years. 
In fact, when talking about the computation of AI tasks at the edge of the network, the literature shows solutions that rely on well-known infrastructures that involve devices, edge, and cloud systems. 

![gartner-hype-cycle-ai-2021](../../assets/images/gartner-hype-cycle-ai-2021.png)
[Figure 1: The 4 Trends That Prevail on the Gartner Hype Cycle for AI, 2021](https://www.gartner.com/en/articles/the-4-trends-that-prevail-on-the-gartner-hype-cycle-for-ai-2021-)

### Communication, Coordination, Cooperation, and Collaboration


0.  Lorenzo Carnevale, Massimo Villari. "<i>A Nature-Inspired Coordination System to Decentralize Intelligence at the Disconnected Edge</i>". 2022 7th IEEE Cyber Science and Technology Congress (CyberSciTech), Falerna (CZ), Italy, September 2022 [[poster](https://drive.google.com/file/d/1mfqEl_vW5i_8IJl8kc6gmqlJYM0uF7Ng/view?usp=sharing)]
{: reversed="reversed"}
-->


#### Selected Publications in Trustworthy AI:  
- <span style="color:blue">[ACSAC'24]</span> Z. Zhang, X. Zhang, Y. Zhang, L. Y. Zhang, C. Chen, S. Hu, A. Gill, S. Pan, "Stealing Watermarks of Large Language Models via Mixed Integer Programming", in ACSAC, 2024. (core a, ccf b) [PDF](https://arxiv.org/abs/2405.19677)
- <span style="color:blue">[ICML'24]</span> L. Hou, R. Feng, Z. Hua, W. Luo, L. Y. Zhang, Y. Li, "IBD-PSC: Input-level Backdoor Detection via Parameter-oriented Scaling Consistency," in ICML, 2024. (core a*, ccf a) [PDF](https://arxiv.org/abs/2405.09786)
- <span style="color:blue">[Oakland'24a]</span>  Y. Zhang, S. Hu, L. Y. Zhang, J. Shi, M. Li, X. Liu, W. Wan, H. Jin, "Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training," in Oakland, 2024. [PDF](https://arxiv.org/abs/2307.07873) [CODE](https://github.com/CGCL-codes/TransferAttackSurrogates) [SLIDES]({{site.baseurl}}/assets/slides/advtransfer.pdf)
- <span style="color:blue">[Oakland'24b]</span>  X. Mo, Y. Zhang, L. Y. Zhang, W. Luo, N. Sun, S. Hu, S. Gao, Y. Xiang, "Robust Backdoor Detection for Deep Learning via Topological Evolution Dynamics," in Oakland, 2024. [PDF](https://arxiv.org/abs/2312.02673) [CODE](https://github.com/tedbackdoordefense/ted) [SLIDES]({{site.baseurl}}/assets/slides/ted.pdf)
- <span style="color:blue">[Oakland'24c]</span>  Z. Zhou, M. Li, W. Liu, S. Hu, Y. Zhang, W. Wan, L. Xue, L. Y. Zhang, D. Yao, H. Jin, "Securely Fine-tuning Pre-trained Encoders Against Adversarial Examples," in Oakland, 2024. [PDF](https://arxiv.org/abs/2403.10801) [CODE](https://github.com/CGCL-codes/Gen-AF)
- <span style="color:blue">[IJCAI'24a]</span>  H. Zhang, S. Hu, Y. Wang, L. Y. Zhang, Z. Zhou, X. Wang, Y. Zhang, C. Chen,  "Detector Collapse: Backdooring Object Detection to Catastrophic Overload or Blindness," in IJCAI, 2024. [PDF](https://arxiv.org/abs/2404.11357) [CODE]() [DEMO](https://object-detection-backdoor.github.io/demo/)
- <span style="color:blue">[IJCAI'24b]</span>  W. Wan, Y. Ning, S. Hu, L. Xue, M. Li, L. Y. Zhang, Y. Wang, "DarkFed: A Data-Free Backdoor Attack in Federated Learning," in IJCAI, 2024. [PDF](https://arxiv.org/pdf/2405.03299) [CODE]()
- <span style="color:blue">[AAA'24a]</span>  Q. Duan, Z. Hua, Q. Liao, Y. Zhang, L. Y. Zhang, "Conditional Backdoor Attack via JPEG Compression," in AAAI, 2024. [PDF](https://ojs.aaai.org/index.php/AAAI/article/view/29957) [CODE]()
- <span style="color:blue">[AAAI'24b]</span>  D. Mi, Y. Zhang, L. Y. Zhang, S. Hu, Q. Zhong, H. Yuan, S. Pan, "Towards Model Extraction Attacks in GAN-based Image Translation via Domain Shift Mitigation," in AAAI, 2024. [PDF]() [CODE]()
- <span style="color:blue">[AAAI'24c]</span>  L. Xue, S. Hu, R. Zhao, L. Y. Zhang, S. Hu, L. Sun, D. Yao, "Revisiting Gradient Pruning: A Dual Realization for Defending Against Gradient Attacks," in AAAI, 2024. [PDF](https://ojs.aaai.org/index.php/AAAI/article/view/29966) [CODE]()
- <span style="color:blue">[Acm MM'23]</span>  W. Wan, S. Hu, M. Li, J. Lu, L. Zhang, L. Y. Zhang, H. Jin, "A Four-Pronged Defense Against Byzantine Attacks in Federated Learning," in Acm MM, 2023. [PDF](https://arxiv.org/abs/2308.03331)
- <span style="color:blue">[ICCV'23]</span>  Z. Zhou, S. Hu, R. Zhao, Q. Wang, L. Y. Zhang, J. Hou, H. Jin, "Downstream-agnostic Adversarial Examples," in ICCV, 2023. [PDF](https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_Downstream-agnostic_Adversarial_Examples_ICCV_2023_paper.html)
- <span style="color:blue">[IJCAI'23]</span>  H. Zhang, Z. Yao, L. Y. Zhang, S. Hu, C. Chen, A. W.-C. Liew, Z. Li, "Denial-of-Service or Fine-Grained Control: Towards Flexible Model Poisoning Attacks on Federated Learning," in IJCAI, 2023. 
- <span style="color:blue">[Asia CCS'23a]</span>  X. Zhang, Z. Zhang, Q. Zhong, X. Zheng,  Y. Zhang,  S. Hu, L. Y. Zhang, "Masked Language Model Based Textual Adversarial Example Detection," in AsiaCCS, 2023. [PDF](https://arxiv.org/pdf/2304.10783)
- <span style="color:blue">[Asia CCS'23b]</span>  M. Ma, Y. Zhang, P. C. M. Arachchige, L. Y. Zhang, M. B. Chhetri, G. Bai, "LoDen: Making Every Client in Federated Learning a Defender Against the Poisoning Membership Inference Attacks," in AsiaCCS, 2023. [PDF](https://dl.acm.org/doi/abs/10.1145/3579856.3590334?casa_token=qKHQIjFsOkEAAAAA:J8VWvI7iG7mnW0fii0U-WGcEc76IYhcfKJyBRI7veoOvKabrMcW6ZYrklEWP0DlxnfdcQDGDuCSJog)
- <span style="color:blue">[TIFS'23]</span>  Z. Gong, L. Shen, Y. Zhang, L. Y. Zhang, J. Wang, G. Bai, Y. Xiang, "AGRAMPLIFIER: Defending Federated Learning Against Poisoning Attacks Through Local Update Amplification," IEEE Transactions on Information Forensics and Security, 2023.
- <span style="color:blue">[CVPR'22]</span>  S. Hu, X. Liu, Y. Zhang, M. Li, L. Y. Zhang, H. Jin, L. Wu, "Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer," in IEEE/CVF CVPR, 2022.
- <span style="color:blue">[IJCAI'22]</span>  W. Wan, S. Hu, J. Lu, L. Y. Zhang, H. Jin, Y. He, "Shielding Federated Learning: Robust Aggregation with Adaptive Client Selection," in IJCAI, 2022.

<!--
Given the critical importance of trust and transparency in AI technologies, addressing security and privacy challenges associated with AI deployment becomes paramount. My research in this area focuses on the following several key aspects: 
- Understanding and improving adversarial robustness <span style="color:blue"> [[Oakland'24a]({{site.baseurl}}/research/trustworthyAI)], [[Oakland'24c]({{site.baseurl}}/research/trustworthyAI)], [[ICCV'23]({{site.baseurl}}/research/trustworthyAI)], [[Asia CCS'23a]({{site.baseurl}}/research/trustworthyAI)], [[CVPR'22]({{site.baseurl}}/research/trustworthyAI)] </span>  
- Designing and defeating poisoning attacks <span style="color:blue"> [[IJCAI'24a]({{site.baseurl}}/research/trustworthyAI)], [[IJCAI'24b]({{site.baseurl}}/research/trustworthyAI)], [[AAAI'24a]({{site.baseurl}}/research/trsutworthyAI)], [[Oakland'24b]({{site.baseurl}}/research/trustworthyAI)], [[Acm MM'23]({{site.baseurl}}/research/trustworthyAI)], [[IJCAI'23]({{site.baseurl}}/research/trustworthyAI)], [[IJCAI'22]({{site.baseurl}}/research/trustworthyAI)] </span>  
- Preventing privacy and IP leakage <span style="color:blue"> [[AAAI'24b]({{site.baseurl}}/research/trustworthyAI)], [[AAAI'24c]({{site.baseurl}}/research/trustworthyAI)], [[Asia CCS'23b]({{site.baseurl}}/research/trustworthyAI)] </span>  
- Improving overall performance in the presence of adversaries <span style="color:blue"> [[TIFS'23]({{site.baseurl}}/research/trustworthyAI)] </span>  
-->